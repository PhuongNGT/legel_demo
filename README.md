# 🧾 LegalStories Evaluation Web App

This Flask-based web app allows human evaluators to assess stories generated by various large language models (LLMs) based on legal doctrines. The system includes:

- Story generation and display from different models (GPT-4, GPT-3.5, etc.)
- A multiple-choice question (MCQ) for each doctrine
- Evaluation forms for subjective metrics (likeability, believability)
- Visualization of results in real-time

---

## 📁 Project Structure

legel/
│
├── data/
│   └── evaluations.csv        # File kết quả đánh giá từ user
│   ├── evaluations.csv         ✅ file người dùng đánh giá cuối cùng
│    ├── legal_doctrines_294.xlsx ✅ doctrines ban đầu
│
│
├── outputs/
│   └── merged_mcq.tsv         # Dữ liệu sinh từ mô hình (merge story + MCQ)
│   └── stories, MCQs, plots
├── static/
│   └── plots/                 # Lưu ảnh được generate từ analyze.py
│
├── templates/
│   ├── index.html
│   ├── evaluate.html
│   └── result.html
│
├── crawler/                   # Crawling legal doctrines từ Wikipedia
│   └── complex-law-doctrine-list.csv
│   └── wiki_crawler.py
│
├── app.py                     # Flask app chính
├── analyze.py                 # Phân tích đánh giá (gồm cả hình ảnh)
├── run_all_models.py          # Chạy tất cả các mô hình sinh truyện
├── check_env.py               # Kiểm tra môi trường
├── .env                       # Thông số môi trường (key, config)
├── requirements.txt
├── README.md
├── model.py               # Load mô hình
├── generate_mcq.py        # Sinh MCQ cho mỗi doctrine
├── merge_stories.py       # Hợp nhất truyện các mô hình
├── visualize.py           # Hiển thị kết quả nâng cao (Table 4)
└── evaluate_stories.py    # Đánh giá điểm like/believable
## 🚀 How to Run

1. **Install dependencies**

```bash
pip install -r requirements.txt
